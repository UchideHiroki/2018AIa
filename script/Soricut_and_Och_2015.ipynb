{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soricut_and_Och_2015の論文を追試するコード  \n",
    "分散表現(Word Embedding WE)化は一旦Gloveで済ませ、\n",
    "input  \n",
    "分散表現  \n",
    "学習用単語群  \n",
    "\n",
    "output  \n",
    "(word1, word2)を入力するとspearman_corrを返すclass \n",
    "\n",
    "を作る  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装手順  \n",
    "1. 語彙集合から全てのルール候補を抽出  \n",
    "    a. prefix or suffixが6文字以下のルールを総当たりで調べる\n",
    "2. 分散表現を学習  \n",
    "3. ルール候補を評価  \n",
    "    a. ルール毎に適用可能な(w1, w2)のsetS_rを求める  \n",
    "    b. S_rを最大1000pairにサンプリング  \n",
    "    c. S_rのpair(w, w')毎に、残り全てのpairに対してcosine距離と  \n",
    "    w1+(w'-w)に対してw2が何番目に近いか, rankを計算  \n",
    "    d. S_rのpair(w, w')毎に、hit_rateを計算  \n",
    "    残り全pairに対して何%が100位以内に収まったか  \n",
    "4. 同一のルールに対し、方向ベクトルを一般化  \n",
    "    a. ルール毎に、最も多くの単語をsupportする, rankが100以下になる(w, w')を選択  \n",
    "    b. S_rから(w, w')がsupportするpairを削除  \n",
    "    c. (w, w')がsupportするpair数が10以下になるまでa→bを繰り返す  \n",
    "    d. transformation: ('suffix':ε:d:↑dethrone)を求める\n",
    "5. 4で求めたsupport setを選別  \n",
    "    a. t_rank < 30, t_cosine > 0.5 \n",
    "6. グラフを作成  \n",
    "    a. 語彙集合全ての語彙をNodeとして格納  \n",
    "    b. 選別に残った(w1, w2), weights=(rank, cosine)をedgeに追加\n",
    "7. グラフを正規化  \n",
    "    a. w2の方が単語数が多くなるようにedgeを刈り取り  \n",
    "    b. それでも多重なら, rankが小さい方のedgeを残す  \n",
    "    c. それでも多重なら、cosineが小さい方のedgeを残す  \n",
    "\n",
    "Moprhを用いた評価手順  \n",
    "入力: (w1, w2)  \n",
    "出力: Spearman ρ correlation、-1 < ρ < 1\n",
    "1. w1, w2がWEにあれば、分散表現を用いて計算  \n",
    "2. wが片方しか無い場合、正規化したグラフでもう片方のwの分散表現を推定する  \n",
    "    a. 存在しない方のw(仮にxとする)に全てのtransformationを適用しx'を求める  \n",
    "    b. x'がWEに存在する単語になった場合、最も出現数の多い物を採用する  \n",
    "    c. 採用したyとtransformationの分散表現を用いてxを計算する  \n",
    "3. 計算したw1, w2で計算  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     74,
     77,
     82,
     85,
     106,
     123,
     139,
     182,
     205,
     239,
     253,
     273
    ]
   },
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    \n",
    "    \n",
    "    def __init__(self, embedding):\n",
    "        self.embedding = embedding\n",
    "        self.rules = None\n",
    "        self.transformations = None\n",
    "        self.transformations_evaluated = None\n",
    "        self.graph = None\n",
    "        self.graph_normalized = None\n",
    "        \n",
    "    \n",
    "    def unicodeToAscii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "        \n",
    "    # 大文字を全部小文字にする\n",
    "    # 無駄な空白や文字じゃないやつを全部消す\n",
    "    def normalizeString(self, s):\n",
    "        s = self.unicodeToAscii(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\"\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\"\", s)\n",
    "        return s\n",
    "    \n",
    "    def extract_rule(self, word1, word2, max_len=6):\n",
    "        # make rule_dict\n",
    "        rules = defaultdict(list)\n",
    "        # check suffix\n",
    "        i = 1\n",
    "        while (word1[:i] == word2[:i]):\n",
    "            i += 1\n",
    "        if i != 1and i > max(len(word1[i-1:]), len(word2[i-1:])) <= max_len:\n",
    "            rules[('suffix', word1[i-1:], word2[i-1:])].append((word1, word2))\n",
    "            \n",
    "        # check prefix\n",
    "        i = 1\n",
    "        while (word1[-i:] == word2[-i:]):\n",
    "            i += 1\n",
    "        if i != 1 and i > max(len(word1[:-i+1]), len(word2[:-i+1])) <= max_len:\n",
    "            rules[('prefix', word1[:-i+1], word2[:-i+1])].append((word1, word2))\n",
    "        return rules\n",
    "    \n",
    "    def extract_rule_wrapped(self, args):\n",
    "        return self.extract_rule(*args)\n",
    "      \n",
    "    def extract_rules(self, vocab, read=True, save=False, file_name=None):\n",
    "        '''\n",
    "        Extract candidate prefix/suffix rules from V.\n",
    "        \n",
    "        input\n",
    "        vocab(set): set of words \n",
    "        real(bool):\n",
    "        save(bool):  \n",
    "        file_name: hogehoge.pickle    '../preprocessed/FILE_NAME'\n",
    "        \n",
    "        output\n",
    "        self.rules(dict): \n",
    "            key(tuple): ('prefix/suffix', 'ing', 'ed')\n",
    "            value(list): list of word_pair (word1, word2) supported by rule\n",
    "        '''\n",
    "        if file_name:\n",
    "            path = Path.cwd().parent.joinpath('preprocessed/{}'.format(file_name))\n",
    "        else:\n",
    "            path = Path.cwd().parent.joinpath('preprocessed/rules.pickle')\n",
    "        if path.exists() and read:\n",
    "            with path.open('rb') as f:\n",
    "                self.rules = pickle.load(f)\n",
    "        else:\n",
    "            rules_full = defaultdict(list)\n",
    "            args_list = [args for args in tqdm(permutations(vocab, 2))]\n",
    "            p = Pool()\n",
    "            rules_list = p.map(self.extract_rule_wrapped, args_list)\n",
    "            [rules_full.update(rules) for rules in rules_list]\n",
    "            self.rules = rules_full\n",
    "            del rules_full\n",
    "            if save:\n",
    "                with path.open('wb') as f:\n",
    "                    pickle.dump(self.rules, f)\n",
    "                \n",
    "    def downsample_rules(self, n_sample=1000):\n",
    "        '''\n",
    "        downsample the number of word pair in suppor set of each rule.\n",
    "        '''\n",
    "        for key, value in self.rules.items():\n",
    "            if len(value) >= n_sample:\n",
    "                self.rules[key] = sample(value, n_sample)\n",
    "    \n",
    "    def is_word_pairs_similar(self, word_pair1, word_pair2, annoy_index=None, topn=10):\n",
    "        '''\n",
    "        Calculate if \n",
    "            word2_1 + (word1_2 - word1_1)\n",
    "        is similar to word2_2\n",
    "        \n",
    "        input\n",
    "            word_pair1(tuple): (word1_1, word1_2), used as direction vector\n",
    "            word_pair2(tuple): (word2_1, word2_2)\n",
    "            \n",
    "        output\n",
    "            bool\n",
    "        '''\n",
    "        closest_n = self.embedding.most_similar(\n",
    "            positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]],\n",
    "                                                                         topn=topn, indexer=annoy_index)\n",
    "        for word, cos_sim in closest_n:\n",
    "            if word == word_pair2[1]:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_similarity_rank(self, word_pair1, word_pair2, topn=100):\n",
    "        '''\n",
    "        Calculate similarity rank and cosine similarity\n",
    "        \n",
    "        input\n",
    "            word_pair1(tuple): direction vector\n",
    "            word_pair2(tuple): word pair to calculate similarity\n",
    "        return\n",
    "            tuple (rank, cos_sim)\n",
    "        '''\n",
    "        closest_n = self.embedding.most_similar(\n",
    "            positive=[word_pair2[0], word_pair1[1]], negative=[word_pair1[0]], topn=topn)\n",
    "        for rank, (word, cos_sim) in enumerate(closest_n):\n",
    "            if word == word_pair2[1]:\n",
    "                return (rank+1, cos_sim) # add 1 to the highest rank to 1, not 0\n",
    "        return (None, None)\n",
    "    \n",
    "    def index_vector(self, dimensions=300, save=False):\n",
    "        '''\n",
    "        make annoy_index which is used in function 'is_word_pairs_similar'\n",
    "        Using annoy_index, execution may be slower than normal index\n",
    "        '''\n",
    "        path = Path.cwd().parent.joinpath('preprocessed/annoy.index')\n",
    "        if path.exists():\n",
    "            annoy_index = AnnoyIndexer()\n",
    "            annoy_index.load(str(path))\n",
    "            annoy_index.model = self.embedding\n",
    "        else:\n",
    "            annoy_index = AnnoyIndexer(self.embedding, dimensions)\n",
    "            if save:\n",
    "                annoy_index.save(str(path))\n",
    "        return annoy_index\n",
    "    \n",
    "    def get_best_transformation(self, rule, support_set, topn=100, return_hit_rate=False):\n",
    "        '''\n",
    "        Get the most supportable transformation from the rule.\n",
    "    \n",
    "        input\n",
    "            rule(tuple): (type, from, to)\n",
    "            support_set(set): {(word1, word2), ...}\n",
    "            topn(int): compute rank and cos_sim if rank is under topn\n",
    "    \n",
    "        return\n",
    "            transformation(tuple): ('suffix', 'ing', 'ed', w, w')\n",
    "            trans_support_set(set): {((word1, word2), rank, cosine), ...}\n",
    "            hit_rate(real): optional                 \n",
    "        '''\n",
    "        # Calculate all the transformations\n",
    "        transformations = defaultdict(tuple)\n",
    "        for word_pair1 in support_set:\n",
    "            transformation = rule + word_pair1  # tuple(type, from, to, w, w')\n",
    "            trans_support_set = set()\n",
    "            n_hit = 0\n",
    "            for word_pair2 in support_set:\n",
    "                if word_pair2 != word_pair1:\n",
    "                    (rank, cos_sim) = self.get_similarity_rank(word_pair1, word_pair2, topn=topn)\n",
    "                    trans_support_word = (word_pair2, rank, cos_sim)\n",
    "                    if rank is not None:\n",
    "                        n_hit += 1\n",
    "                        trans_support_set.add(trans_support_word)\n",
    "            # avoid 0 division\n",
    "            hit_rate = n_hit / \\\n",
    "                len(trans_support_set) if len(trans_support_set) != 0 else 0\n",
    "            transformations[transformation] = (hit_rate, trans_support_set)\n",
    "    \n",
    "        # Select hit_rate-highest one\n",
    "        transformations_by_count = sorted(\n",
    "            transformations.items(), key=lambda kv: kv[1][0], reverse=True)\n",
    "        best_transformation, (best_hit_rate,\n",
    "                              trans_support_set) = transformations_by_count[0]\n",
    "    \n",
    "        if return_hit_rate:\n",
    "            return best_transformation, trans_support_set, best_hit_rate\n",
    "        else:\n",
    "            return best_transformation, trans_support_set\n",
    "\n",
    "    def generate_transformations(self, topn=100, min_explain_count=10):\n",
    "        '''\n",
    "        Generate transformations and their support_sets\n",
    "        \n",
    "        return\n",
    "            transformations(dict):\n",
    "                key: transformation (type, from, to, word1, word2)\n",
    "                value: transformation-support set {((word1, word2), rank, cosine), ...}\n",
    "        '''\n",
    "        transformations = defaultdict(dict)\n",
    "        for rule, support_set in tqdm(self.rules.items()):\n",
    "            support_set = set(support_set)\n",
    "            while True:\n",
    "                (tran, tran_support_set) = self.get_best_transformation(rule, support_set, topn=topn)\n",
    "                if len(tran_support_set) < min_explain_count:\n",
    "                    break\n",
    "                transformations[tran] = tran_support_set\n",
    "                tran_support_words = {word[0] for word in tran_support_set}\n",
    "                support_set = support_set - {tran[3:5]} - tran_support_words\n",
    "                if len(support_set) < min_explain_count:\n",
    "                    break\n",
    "        self.transformations = transformations\n",
    "    \n",
    "    def generate_trans(self, rule, support_set, topn, min_explain_count):\n",
    "        trans_on_rule = defaultdict(dict)\n",
    "        support_set = set(support_set)\n",
    "        while True:\n",
    "            (tran, tran_support_set) = self.get_best_transformation(\n",
    "                rule, support_set, topn=topn)\n",
    "            if len(tran_support_set) < min_explain_count:\n",
    "                break\n",
    "            trans_on_rule[tran] = tran_support_set\n",
    "            tran_support_words = {word[0] for word in tran_support_set}\n",
    "            support_set = support_set - {tran[3:5]} - tran_support_words\n",
    "            if len(support_set) < min_explain_count:\n",
    "                break\n",
    "        return trans_on_rule\n",
    "\n",
    "    def generate_trans_wrapped(self, args):\n",
    "        return self.generate_trans(*args)\n",
    "\n",
    "    def generate_transformations_parallel(self, topn=100, min_explain_count=10):\n",
    "        '''\n",
    "        Generate transformations and their support_sets in parallel\n",
    "            \n",
    "        return\n",
    "            transformations(dict):\n",
    "                key: transformation (type, from, to, word1, word2)\n",
    "                value: transformation-support set {((word1, word2), rank, cosine), ...}\n",
    "        '''\n",
    "        transformations = defaultdict(dict)    \n",
    "        args =[(rule, support_set, topn, min_explain_count) for rule, support_set in self.rules.items()]\n",
    "        p = Pool() # processesを指定しなければ、cpuの指定出来る最大コア数を使う\n",
    "        trans_list = p.map(self.generate_trans_wrapped, args)\n",
    "        [transformations.update(trans) for trans in trans_list]\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def evaluate_transformations(self, rank=30, cos_sim=0.5):\n",
    "        '''\n",
    "        Evaluate how well it passes a proximity test in embedding space.\n",
    "        \n",
    "        return:\n",
    "            self.evaluated_transformations\n",
    "        '''\n",
    "        trans = deepcopy(self.transformations)\n",
    "        for tran, support_set in tqdm(self.transformations.items()):\n",
    "            for word in support_set:\n",
    "                if word[1] > rank or word[2] < cos_sim: # cos_simの制約で結構落ちる\n",
    "                    trans[tran].remove(word)\n",
    "        self.transformations_evaluated = trans\n",
    "        \n",
    "    def build_graph(self, is_use_trans_evaluated=True):\n",
    "        '''\n",
    "        build Graph and add Nodes and Edges to self.graph\n",
    "    \n",
    "        input:\n",
    "            is_use_trans_evaluated(bool)\n",
    "        '''\n",
    "        G = nx.MultiDiGraph()\n",
    "        G.add_nodes_from(self.embedding.vocab.keys())\n",
    "        if is_use_trans_evaluated:\n",
    "            transformations = self.transformations_evaluated\n",
    "        else:\n",
    "            transformations = self.transformations\n",
    "        for dw, support in tqdm(transformations.items()):\n",
    "            # word_pair = dw so assign 0 for rank\n",
    "            G.add_edge(dw[3], dw[4], transformation=dw, rank=0, cos_sim=1)\n",
    "            for ((word1, word2), rank, cos_sim) in support:\n",
    "                G.add_edge(word1, word2, transformation=dw, rank=rank, cos_sim=cos_sim)\n",
    "        self.graph = G\n",
    "        \n",
    "    def normalize_graph(self):\n",
    "        '''\n",
    "        convert multi-directed self.graphraph to directed self.graphraph\n",
    "        '''\n",
    "        graph = deepcopy(self.graph)\n",
    "        for node in tqdm(self.graph.nodes()):\n",
    "            for neighbor in self.graph.neighbors(node):\n",
    "                # if node and neighbor do not have multiple edges, continue\n",
    "                if not (self.graph.has_edge(node, neighbor) and self.graph.has_edge(neighbor, node)):\n",
    "                    continue\n",
    "                # Delete one of the edges\n",
    "                # edge w1→w2 considered only if count(w1) < count(w2)\n",
    "                if self.embedding.vocab[node].count > self.embedding.vocab[neighbor].count:\n",
    "                    graph.remove_edge(node, neighbor)\n",
    "                    continue\n",
    "                # chose the one with minimal rank\n",
    "                if self.graph[node][neighbor][0]['rank'] > self.graph[neighbor][node][0]['rank']:\n",
    "                    graph.remove_edge(node, neighbor)\n",
    "                    continue\n",
    "                # chose the one with the maximal cosine\n",
    "                if self.graph[node][neighbor][0]['cos_sim'] < self.graph[neighbor][node][0]['cos_sim']:\n",
    "                    graph.remove_edge(node, neighbor)\n",
    "                    continue\n",
    "            self.graph_normalized = graph\n",
    "            \n",
    "    def get_morph_trans(self, use_normalized_graph=True):\n",
    "        '''\n",
    "        Extract all transformations from the graph.\n",
    "        \n",
    "        input:\n",
    "            use_normalized_graph(bool): use self.graph_normalized if True or use self.graph\n",
    "        \n",
    "        output:\n",
    "            morph_trans: morphological transformations (type, from, to, w, w')\n",
    "        '''\n",
    "        morph_trans = set()\n",
    "        if use_normalized_graph is True:\n",
    "            for edge in self.graph_normalized.edges(data=True):\n",
    "                morph_trans.add(edge[2]['transformation'])\n",
    "        else:\n",
    "            for edge in self.graph.edges(data=True):\n",
    "                morph_trans.add(edge[2]['transformation'])\n",
    "        return morph_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際の使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディレクトリ構造  \n",
    "script  \n",
    " -このノートブック  \n",
    "input  \n",
    " -GoogleNews  \n",
    "preprocessed  \n",
    " -morph.rulesやmorph.transformationsなど一時的な計算結果を保存  \n",
    "output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = KeyedVectors.load_word2vec_format('../input/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=1000)\n",
    "morph = Morph(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeString('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# あらかじめ入力する単語を綺麗にしておく\n",
    "# 全て小文字にして、a-z以外の単語を取り除く\n",
    "normalized_words = set([morph.normalizeString(word) for word in embedding.vocab.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0836169d28fa4de99511c88fa16293ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "morph.extract_rules(normalized_words, read=False, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('prefix', 'a', 'we'): [('are', 'were')],\n",
       "             ('prefix', 'a', 'mo'): [('are', 'more')],\n",
       "             ('prefix', 'c', 'th'): [('can', 'than')],\n",
       "             ('prefix', 'c', ''): [('can', 'an')],\n",
       "             ('prefix', 'ot', ''): [('other', 'her')],\n",
       "             ('suffix', 'w', 't'): [('now', 'not')],\n",
       "             ('suffix', 'w', ''): [('now', 'no')],\n",
       "             ('prefix', '', 'u'): [('s', 'us')],\n",
       "             ('prefix', '', 'i'): [('s', 'is')],\n",
       "             ('suffix', '', 'o'): [('s', 'so')],\n",
       "             ('prefix', '', 'a'): [('s', 'as')],\n",
       "             ('prefix', 'u', ''): [('us', 's')],\n",
       "             ('prefix', 'u', 'i'): [('us', 'is')],\n",
       "             ('suffix', 's', 'p'): [('us', 'up')],\n",
       "             ('prefix', 'u', 'a'): [('us', 'as')],\n",
       "             ('prefix', 'we', 'a'): [('were', 'are')],\n",
       "             ('suffix', 're', ''): [('there', 'the')],\n",
       "             ('prefix', 'we', 'mo'): [('were', 'more')],\n",
       "             ('prefix', 'w', 'th'): [('were', 'there')],\n",
       "             ('suffix', 't', 'r'): [('out', 'our')],\n",
       "             ('prefix', '', 'ab'): [('out', 'about')],\n",
       "             ('prefix', 'o', 'b'): [('out', 'but')],\n",
       "             ('prefix', 'w', 'c'): [('would', 'could')],\n",
       "             ('prefix', 'bef', 'm'): [('before', 'more')],\n",
       "             ('suffix', 'd', 've'): [('had', 'have')],\n",
       "             ('suffix', 'd', 's'): [('had', 'has')],\n",
       "             ('suffix', '', 's'): [('year', 'years')],\n",
       "             ('suffix', '', 't'): [('no', 'not')],\n",
       "             ('suffix', '', 'n'): [('a', 'an')],\n",
       "             ('suffix', '', 'f'): [('i', 'if')],\n",
       "             ('prefix', 'i', ''): [('is', 's')],\n",
       "             ('prefix', 'i', 'u'): [('is', 'us')],\n",
       "             ('suffix', 's', ''): [('as', 'a')],\n",
       "             ('prefix', '', 'h'): [('as', 'has')],\n",
       "             ('suffix', 's', 't'): [('as', 'at')],\n",
       "             ('suffix', 's', 'n'): [('as', 'an')],\n",
       "             ('prefix', 'i', 'a'): [('in', 'an')],\n",
       "             ('prefix', '', 'th'): [('an', 'than')],\n",
       "             ('suffix', 's', 'f'): [('is', 'if')],\n",
       "             ('suffix', 't', 'w'): [('not', 'now')],\n",
       "             ('suffix', 't', ''): [('at', 'a')],\n",
       "             ('prefix', 'be', 'wh'): [('been', 'when')],\n",
       "             ('suffix', 'en', ''): [('been', 'be')],\n",
       "             ('prefix', 'h', ''): [('has', 'as')],\n",
       "             ('suffix', 's', 'm'): [('his', 'him')],\n",
       "             ('prefix', '', 't'): [('he', 'the')],\n",
       "             ('prefix', 'ju', 'la'): [('just', 'last')],\n",
       "             ('suffix', 'p', 's'): [('up', 'us')],\n",
       "             ('prefix', 's', ''): [('she', 'he')],\n",
       "             ('prefix', 's', 't'): [('she', 'the')],\n",
       "             ('prefix', 'd', 'sa'): [('did', 'said')],\n",
       "             ('suffix', 't', 's'): [('at', 'as')],\n",
       "             ('suffix', 't', 'n'): [('that', 'than')],\n",
       "             ('suffix', 't', 'f'): [('it', 'if')],\n",
       "             ('suffix', 'ey', 'an'): [('they', 'than')],\n",
       "             ('suffix', 'y', 'm'): [('they', 'them')],\n",
       "             ('suffix', 'y', 'ir'): [('they', 'their')],\n",
       "             ('suffix', 'ey', 'at'): [('they', 'that')],\n",
       "             ('suffix', 'y', ''): [('they', 'the')],\n",
       "             ('suffix', 'ey', 'is'): [('they', 'this')],\n",
       "             ('suffix', 'y', 're'): [('they', 'there')],\n",
       "             ('suffix', 'r', 't'): [('our', 'out')],\n",
       "             ('prefix', 'd', 's'): [('do', 'so')],\n",
       "             ('prefix', 'd', 'n'): [('do', 'no')],\n",
       "             ('suffix', 'll', 'th'): [('will', 'with')],\n",
       "             ('prefix', 'wi', 'a'): [('will', 'all')],\n",
       "             ('suffix', 'm', 's'): [('him', 'his')],\n",
       "             ('prefix', 'wh', 'be'): [('when', 'been')],\n",
       "             ('suffix', 'en', 'o'): [('when', 'who')],\n",
       "             ('suffix', 'en', 'at'): [('when', 'what')],\n",
       "             ('prefix', 'f', ''): [('for', 'or')],\n",
       "             ('prefix', 'w', ''): [('was', 'as')],\n",
       "             ('prefix', 'w', 'h'): [('we', 'he')],\n",
       "             ('suffix', 'o', 'en'): [('who', 'when')],\n",
       "             ('suffix', 'o', 'at'): [('who', 'what')],\n",
       "             ('prefix', 'th', 'c'): [('than', 'can')],\n",
       "             ('suffix', 'an', 'ey'): [('than', 'they')],\n",
       "             ('suffix', 'an', 'em'): [('than', 'them')],\n",
       "             ('suffix', 'n', 't'): [('an', 'at')],\n",
       "             ('suffix', 'an', 'e'): [('than', 'the')],\n",
       "             ('suffix', 'an', 'is'): [('than', 'this')],\n",
       "             ('prefix', 'th', ''): [('this', 'is')],\n",
       "             ('prefix', 'sa', 'd'): [('said', 'did')],\n",
       "             ('prefix', '', 'ot'): [('her', 'other')],\n",
       "             ('prefix', 'h', 'ov'): [('her', 'over')],\n",
       "             ('suffix', 'r', ''): [('her', 'he')],\n",
       "             ('suffix', 'e', 'ly'): [('one', 'only')],\n",
       "             ('suffix', 'e', ''): [('one', 'on')],\n",
       "             ('suffix', 'm', 'y'): [('them', 'they')],\n",
       "             ('suffix', 'em', 'an'): [('them', 'than')],\n",
       "             ('suffix', 'm', 'ir'): [('them', 'their')],\n",
       "             ('suffix', 'em', 'at'): [('them', 'that')],\n",
       "             ('suffix', 'm', ''): [('them', 'the')],\n",
       "             ('suffix', 'em', 'is'): [('them', 'this')],\n",
       "             ('suffix', 'm', 're'): [('them', 'there')],\n",
       "             ('suffix', 'ly', 'e'): [('only', 'one')],\n",
       "             ('suffix', 'ly', ''): [('only', 'on')],\n",
       "             ('suffix', 'o', ''): [('so', 's')],\n",
       "             ('prefix', 's', 'd'): [('so', 'do')],\n",
       "             ('suffix', '', 'me'): [('so', 'some')],\n",
       "             ('prefix', 's', 'n'): [('so', 'no')],\n",
       "             ('prefix', '', 'al'): [('so', 'also')],\n",
       "             ('suffix', 'me', ''): [('some', 'so')],\n",
       "             ('prefix', 'so', 'ti'): [('some', 'time')],\n",
       "             ('prefix', 'so', 'ga'): [('some', 'game')],\n",
       "             ('prefix', 'ti', 'so'): [('time', 'some')],\n",
       "             ('prefix', 'ti', 'ga'): [('time', 'game')],\n",
       "             ('prefix', '', 'f'): [('or', 'for')],\n",
       "             ('suffix', 'r', 'n'): [('or', 'on')],\n",
       "             ('suffix', 'n', ''): [('an', 'a')],\n",
       "             ('suffix', 'n', 's'): [('an', 'as')],\n",
       "             ('suffix', '', 'to'): [('in', 'into')],\n",
       "             ('prefix', 'i', 'o'): [('in', 'on')],\n",
       "             ('suffix', 'n', 'f'): [('in', 'if')],\n",
       "             ('suffix', '', 'w'): [('no', 'now')],\n",
       "             ('prefix', 'n', 'd'): [('no', 'do')],\n",
       "             ('prefix', 'n', 's'): [('no', 'so')],\n",
       "             ('prefix', 'ov', 'h'): [('over', 'her')],\n",
       "             ('prefix', 'a', ''): [('as', 's')],\n",
       "             ('prefix', 'a', 'u'): [('as', 'us')],\n",
       "             ('prefix', 'a', 'i'): [('an', 'in')],\n",
       "             ('prefix', '', 'w'): [('as', 'was')],\n",
       "             ('prefix', 'c', 'w'): [('could', 'would')],\n",
       "             ('prefix', '', 's'): [('he', 'she')],\n",
       "             ('suffix', '', 'r'): [('he', 'her')],\n",
       "             ('prefix', 'h', 'b'): [('he', 'be')],\n",
       "             ('prefix', 'h', 'w'): [('has', 'was')],\n",
       "             ('suffix', '', 'en'): [('be', 'been')],\n",
       "             ('prefix', 'b', 'h'): [('be', 'he')],\n",
       "             ('prefix', 'b', 'w'): [('be', 'we')],\n",
       "             ('suffix', 'e', 'y'): [('be', 'by')],\n",
       "             ('suffix', '', 're'): [('the', 'there')],\n",
       "             ('prefix', 'w', 'b'): [('we', 'be')],\n",
       "             ('prefix', 'al', ''): [('also', 'so')],\n",
       "             ('suffix', 'so', 'l'): [('also', 'all')],\n",
       "             ('suffix', 'to', ''): [('into', 'in')],\n",
       "             ('suffix', 'ir', 'y'): [('their', 'they')],\n",
       "             ('suffix', 'ir', 'm'): [('their', 'them')],\n",
       "             ('suffix', 'ir', ''): [('their', 'the')],\n",
       "             ('suffix', 'ir', 're'): [('their', 'there')],\n",
       "             ('suffix', 'y', 'e'): [('by', 'be')],\n",
       "             ('prefix', '', 'wh'): [('at', 'what')],\n",
       "             ('suffix', 'at', 'en'): [('what', 'when')],\n",
       "             ('suffix', 'at', 'o'): [('what', 'who')],\n",
       "             ('prefix', 'wh', ''): [('what', 'at')],\n",
       "             ('prefix', 'w', 't'): [('what', 'that')],\n",
       "             ('suffix', 've', 'd'): [('have', 'had')],\n",
       "             ('suffix', 've', 's'): [('have', 'has')],\n",
       "             ('suffix', 'at', 'ey'): [('that', 'they')],\n",
       "             ('suffix', 'at', 'em'): [('that', 'them')],\n",
       "             ('prefix', 't', 'w'): [('that', 'what')],\n",
       "             ('suffix', 'at', 'e'): [('that', 'the')],\n",
       "             ('suffix', 'at', 'is'): [('that', 'this')],\n",
       "             ('suffix', '', 'e'): [('on', 'one')],\n",
       "             ('suffix', '', 'ly'): [('on', 'only')],\n",
       "             ('suffix', 'n', 'r'): [('on', 'or')],\n",
       "             ('prefix', 'o', 'i'): [('on', 'in')],\n",
       "             ('prefix', 'o', 'a'): [('on', 'an')],\n",
       "             ('prefix', 't', 's'): [('the', 'she')],\n",
       "             ('suffix', '', 'y'): [('the', 'they')],\n",
       "             ('suffix', 'e', 'an'): [('the', 'than')],\n",
       "             ('suffix', '', 'm'): [('the', 'them')],\n",
       "             ('prefix', 't', ''): [('this', 'his')],\n",
       "             ('suffix', '', 'ir'): [('the', 'their')],\n",
       "             ('suffix', 'e', 'at'): [('the', 'that')],\n",
       "             ('suffix', 'e', 'is'): [('the', 'this')],\n",
       "             ('prefix', 'ab', ''): [('about', 'out')],\n",
       "             ('suffix', 'is', 'ey'): [('this', 'they')],\n",
       "             ('suffix', 'is', 'an'): [('this', 'than')],\n",
       "             ('suffix', 'is', 'em'): [('this', 'them')],\n",
       "             ('suffix', 'is', 'at'): [('this', 'that')],\n",
       "             ('suffix', 'is', 'e'): [('this', 'the')],\n",
       "             ('prefix', 'mo', 'a'): [('more', 'are')],\n",
       "             ('prefix', 'mo', 'we'): [('more', 'were')],\n",
       "             ('prefix', 'm', 'bef'): [('more', 'before')],\n",
       "             ('suffix', 'th', 'll'): [('with', 'will')],\n",
       "             ('prefix', 'b', 'o'): [('but', 'out')],\n",
       "             ('prefix', 'a', 'wi'): [('all', 'will')],\n",
       "             ('suffix', 'l', 'so'): [('all', 'also')],\n",
       "             ('prefix', 'th', 'w'): [('there', 'were')],\n",
       "             ('suffix', 're', 'y'): [('there', 'they')],\n",
       "             ('suffix', 're', 'm'): [('there', 'them')],\n",
       "             ('suffix', 're', 'ir'): [('there', 'their')],\n",
       "             ('suffix', 's', 'd'): [('has', 'had')],\n",
       "             ('suffix', 's', 've'): [('has', 'have')],\n",
       "             ('suffix', 'f', ''): [('if', 'i')],\n",
       "             ('suffix', 'f', 's'): [('if', 'is')],\n",
       "             ('suffix', 'f', 't'): [('if', 'it')],\n",
       "             ('suffix', 'f', 'n'): [('if', 'in')],\n",
       "             ('prefix', 'la', 'ju'): [('last', 'just')],\n",
       "             ('prefix', 'ga', 'so'): [('game', 'some')],\n",
       "             ('prefix', 'ga', 'ti'): [('game', 'time')],\n",
       "             ('prefix', '', 'c'): [('an', 'can')],\n",
       "             ('prefix', 'a', 'o'): [('an', 'on')]})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrace rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22530b1d94144474bc76ecd8c5cc495a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limitを5000以上にすると計算量が爆発的に増えて処理が終わらなくなる\n",
    "embedding = KeyedVectors.load_word2vec_format('../input/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=100000)\n",
    "morph = Morph(word_vectors)\n",
    "print('Extract rules...')\n",
    "morph.extract_rules(embedding.vocab.keys(), read=True, save=False, file_name='rules_10000.pickle')\n",
    "with open('../preprocessed/rules_100000.pickle', mode='wb') as f:\n",
    "    pickle.dump(morph.rules, f)\n",
    "print('Downsample rules...')\n",
    "morph.downsample_rules()\n",
    "print('Generate transformations...')\n",
    "morph.generate_transformations_parallel(topn=10, min_explain_count=4)\n",
    "with open('./preprocessed/transformations_100000.pickle', mode='wb') as f:\n",
    "    pickle.dump(morph.transformations, f)\n",
    "print('Evaluate transformations...')\n",
    "morph.evaluate_transformations(rank=3, cos_sim=0.5)\n",
    "print('Building graph...')\n",
    "morph.build_graph(is_use_trans_evaluated=True)\n",
    "print('Normalize graph...')\n",
    "morph.normalize_graph()\n",
    "with open('../preprocessed/normalized_graph_100000.pickle', mode='wb') as f:\n",
    "    pickle.dump(morph.graph_normalized, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphを用いたRare Wordsetの評価の仕方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = KeyedVectors.load_word2vec_format('../input/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = Morph(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../preprocessed/rules_100000.pickle', mode='rb') as f:\n",
    "    morph.rules = pickle.load(f)\n",
    "    \n",
    "with open('../preprocessed/transformations_100000.pickle', mode='rb') as f:\n",
    "    morph.transformations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate transformations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f4fc19feec42bdaa3831f248c470ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building graph...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665a1ccaf63441e6b51177cf33b2b851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalize graph...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31d59d665f4434b947633664289f10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate transformations...')\n",
    "morph.evaluate_transformations(rank=3, cos_sim=0.5)\n",
    "print('Building graph...')\n",
    "morph.build_graph(is_use_trans_evaluated=True)\n",
    "print('Normalize graph...')\n",
    "morph.normalize_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_trans = morph.get_morph_trans(use_normalized_graph=True)\n",
    "embedding = morph.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def apply_rule(rule, word):\n",
    "    '''\n",
    "    apply the transformation to the word\n",
    "    rule: (type, from, to)   tran: (type, from, to, w, w')\n",
    "    word: str\n",
    "    '''\n",
    "    if rule[0] == 'suffix' and word[::-1].startswith(rule[1][::-1]):\n",
    "        return word[::-1].replace(rule[1][::-1], rule[2][::-1], 1)[::-1]\n",
    "    elif rule[0] == 'prefix' and word.startswith(rule[1]):\n",
    "        return word.replace(rule[1], rule[2], 1)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_oov_vector(trans, word, embedding):\n",
    "    '''\n",
    "    input\n",
    "        trans: set of (type, from, to, w, w')\n",
    "        word: str, not in V\n",
    "        \n",
    "    output:\n",
    "        word_vec: vector or Nonr\n",
    "        match: True or False\n",
    "    '''\n",
    "    nw = (None, None, 0) # (new_word, tran, count)\n",
    "    for tran in trans:\n",
    "        candidate_word = apply_rule(tran, word) # return word or False\n",
    "        if candidate_word in embedding.vocab.keys():\n",
    "            if embedding.vocab[candidate_word].count > nw[2]:\n",
    "                nw = (candidate_word, tran, embedding.vocab[candidate_word].count)\n",
    "    if nw[0]: # if new_word exists\n",
    "        # print('Hit morph trans: {}→{}'.format(word, nw[0]))\n",
    "        nwv = embedding[nw[0]] - (embedding[nw[1][4]] - embedding[nw[1][3]]) # nw: (new_word, (type, from, to, w, w'), count)\n",
    "        return (nwv, True)\n",
    "    else:\n",
    "        return (None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rw = pd.read_csv('../input/rw.txt', header=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 76 ms, total: 3.1 s\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corr_list = list() # 計算した相関係数を格納するlist\n",
    "for index, row in rw.iterrows():\n",
    "    w1, w2 = row[0], row[1]\n",
    "    try:\n",
    "        cor, p = spearmanr(embedding[w1], embedding[w2])\n",
    "        corr_list.append(cor)\n",
    "    except Exception as e:\n",
    "        if w1 in embedding.vocab:\n",
    "            w2_vec, match = get_oov_vector(morph_trans, w2, embedding)\n",
    "            if match:\n",
    "                cor, p = spearmanr(embedding[w1], w2_vec)\n",
    "                corr_list.append(cor)\n",
    "        elif w2 in embedding.vocab:\n",
    "            w1_vec, match = get_oov_vector(morph_trans, w1, embedding)\n",
    "            if match:\n",
    "                cor, p = spearmanr(w1_vec, embedding[w2])\n",
    "                corr_list.append(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 21.21148513236549\n",
      "All pairs: 2034\n",
      "Evaluated pairs: 1317\n"
     ]
    }
   ],
   "source": [
    "print('Performance: {}'.format(np.mean(corr_list) * 100))\n",
    "print('All pairs: {}'.format(rw.shape[0]))\n",
    "print('Evaluated pairs: {}'.format(len(corr_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パフォーマンスが悪いのは多分分析に使用しているembeddingが小さいため  \n",
    "limitを10000→100000にしたらPerformanceが18→21に上がった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list = list()\n",
    "for index, row in rw.iterrows():\n",
    "    w1, w2 = row[0], row[1]\n",
    "    try:\n",
    "        cor, p = spearmanr(embedding[w1], embedding[w2])\n",
    "        corr_list.append(cor)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 24.419332287358923\n",
      "All pairs: 2034\n",
      "Evaluated pairs: 782\n"
     ]
    }
   ],
   "source": [
    "print('Performance: {}'.format(np.mean(corr_list) * 100))\n",
    "print('All pairs: {}'.format(rw.shape[0]))\n",
    "print('Evaluated pairs: {}'.format(len(corr_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphを使うことで、評価出来るペアの数は増えた  \n",
    "一方で、平均パフォーマンスは下がった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.44 s, sys: 128 ms, total: 4.57 s\n",
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corr_list = list() # 計算した相関係数を格納するlist\n",
    "for index, row in rw.iterrows():\n",
    "    w1, w2 = row[0], row[1]\n",
    "    try:\n",
    "        cor, p = spearmanr(embedding[w1], embedding[w2])\n",
    "        # corr_list.append(cor)\n",
    "    except Exception as e:\n",
    "        if w1 in embedding.vocab:\n",
    "            w2_vec, match = get_oov_vector(morph_trans, w2, embedding)\n",
    "            if match:\n",
    "                cor, p = spearmanr(embedding[w1], w2_vec)\n",
    "                corr_list.append(cor)\n",
    "        elif w2 in embedding.vocab:\n",
    "            w1_vec, match = get_oov_vector(morph_trans, w1, embedding)\n",
    "            if match:\n",
    "                cor, p = spearmanr(w1_vec, embedding[w2])\n",
    "                corr_list.append(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 16.52263190768349\n",
      "All pairs: 2034\n",
      "Evaluated pairs: 535\n"
     ]
    }
   ],
   "source": [
    "print('Performance: {}'.format(np.mean(corr_list) * 100))\n",
    "print('All pairs: {}'.format(rw.shape[0]))\n",
    "print('Evaluated pairs: {}'.format(len(corr_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphで計算できたペアのみだと、Performanceは16.5  \n",
    "デフォルトのembeddingと比べると精度は落ちる  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
