{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision tqdm annoy gensim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from random import sample\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from itertools import permutations, chain\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#from gensim.similarities.index import AnnoyIndexer\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=5000)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "# 大文字を全部小文字にする\n",
    "# 無駄な空白や文字じゃないやつを全部消す\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\"\", s)\n",
    "    return s\n",
    "\n",
    "# あらかじめ入力する単語を綺麗にしておく\n",
    "# 全て小文字にして、a-z以外の単語を取り除く\n",
    "normalized_words = set([normalizeString(word) for word in embedding.vocab.keys()]) & embedding.vocab.keys()\n",
    "\n",
    "n_train = int(len(normalized_words) * 0.9)\n",
    "n_validate = int(len(normalized_words) * 0.05)\n",
    "n_test = len(normalized_words) - n_train - n_validate\n",
    "\n",
    "all_words = list(normalized_words)\n",
    "train_words = set(sample(all_words, n_train))\n",
    "validate_words = set(sample(list(set(all_words) - train_words), n_validate))\n",
    "test_words = normalized_words - train_words - validate_words\n",
    "\n",
    "\n",
    "# モデルパラメータの設定\n",
    "all_letters = string.ascii_lowercase\n",
    "n_letters = len(all_letters)\n",
    "input_size = n_letters\n",
    "        \n",
    "def letter2tensor(letter):\n",
    "    return all_letters.index(letter)\n",
    "\n",
    "def letter2onehot(letter):\n",
    "    tensor = torch.zeros(1, n_letters, device=device)\n",
    "    tensor[0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def word2input_tensors(word):\n",
    "    return torch.tensor([letter2tensor(letter) for letter in word]) * 1.0\n",
    "\n",
    "def word2input_one_hot(word):\n",
    "     return torch.cat([letter2onehot(l) for l in word], dim=0)\n",
    "    \n",
    "def word2target_tensor(word):\n",
    "    return torch.from_numpy(embedding[word]).view(1, -1).to(device)\n",
    "\n",
    "#calc the time\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder_2(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, output_dim):\n",
    "        super(GRUEncoder_2, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(27, self.emb_dim) # a-z, paddingの合計27種類\n",
    "        self.gru = nn.LSTM(self.emb_dim, self.h_dim, batch_first = True, dropout = 0.3)\n",
    "        self.affin_prob_pre = nn.Linear(self.h_dim, 1)\n",
    "        self.threshold = nn.Threshold(0.20, 0)\n",
    "        self.affin = nn.Linear(self.h_dim, self.output_dim)\n",
    "                \n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = torch.zeros(1, b_size, self.h_dim, device=device)\n",
    "        return (h0, h0)\n",
    "    \n",
    "    def forward(self, words, lengths, b_size):\n",
    "        \n",
    "        self.hidden, self.cell = self.init_hidden(words.size(0))\n",
    "        embed = self.emb(words)\n",
    "        lengths = lengths.view(-1)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)      \n",
    "        output, hidden = self.gru(packed_input, (self.hidden, self.cell))\n",
    "        \n",
    "        output = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0]\n",
    "        torch_sum = torch.sum(output, dim = 2)\n",
    "        \n",
    "        output_splitter_prob = F.softmax(torch_sum)\n",
    "        output_weighted = torch.bmm(output_splitter_prob.view(output_splitter_prob.size(0), -1,output_splitter_prob.size(1)), output)\n",
    "        output = self.affin(output_weighted.view(-1,self.h_dim)) \n",
    "        \n",
    "        return output, output_splitter_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder_3(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, output_dim):\n",
    "        super(GRUEncoder_3, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(27, self.emb_dim) # a-z, paddingの合計27種類\n",
    "        self.gru = nn.LSTM(self.emb_dim, self.h_dim, 2, batch_first = True, dropout = 0.3, bidirectional = True)\n",
    "        self.affin_prob_pre = nn.Linear(self.h_dim, 1)\n",
    "        self.threshold = nn.Threshold(0.20, 0)\n",
    "        self.affin = nn.Linear(self.h_dim * 2, self.output_dim)\n",
    "                \n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = torch.zeros(1 * 2 * 2, b_size, self.h_dim, device=device)\n",
    "        return (h0, h0)\n",
    "    \n",
    "    def forward(self, words, lengths, b_size):\n",
    "        \n",
    "        self.hidden, self.cell = self.init_hidden(words.size(0))\n",
    "        embed = self.emb(words)\n",
    "        lengths = lengths.view(-1)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        \n",
    "        #lstm\n",
    "        output, hidden = self.gru(packed_input, (self.hidden, self.cell))\n",
    "        output = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0]\n",
    "        torch_sum = torch.sum(output, dim = 2)\n",
    "        \n",
    "        output_splitter_prob = F.softmax(torch_sum)\n",
    "        output_splitter_prob = self.threshold(F.relu(output_splitter_prob))\n",
    "        output_weighted = torch.bmm(output_splitter_prob.view(output_splitter_prob.size(0), -1,output_splitter_prob.size(1)), output)\n",
    "        output = self.affin(output_weighted.view(-1,self.h_dim * 2)) \n",
    "        \n",
    "        return output, output_splitter_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder_4(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, output_dim):\n",
    "        super(GRUEncoder_4, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.middle_dim = 100 # middle dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(27, self.emb_dim) # a-z, paddingの合計27種類\n",
    "        self.gru = nn.LSTM(self.emb_dim, self.h_dim, 2, batch_first = True, dropout = 0.3, bidirectional = True)\n",
    "        self.affin_middle = nn.Linear(self.h_dim * 2, self.middle_dim * 2)\n",
    "        self.affin = nn.Linear(self.middle_dim * 2, 1)\n",
    "        self.threshold = nn.Threshold(0.2, 0.0)\n",
    "        self.last_affin = nn.Linear(self.h_dim * 2, self.h_dim)\n",
    "        \n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = torch.zeros(1 * 2 * 2, b_size, self.h_dim, device=device)\n",
    "        return (h0, h0)\n",
    "    \n",
    "    def forward(self, words, lengths, b_size):\n",
    "        \n",
    "        self.hidden, self.cell = self.init_hidden(words.size(0))\n",
    "        embed = self.emb(words)\n",
    "        lengths = lengths.view(-1)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        \n",
    "        #lstm\n",
    "        output, hidden = self.gru(packed_input, (self.hidden, self.cell))\n",
    "        output_lstm = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0] # batch * seq_len * hidden*2\n",
    "        \n",
    "        output_middle = self.affin_middle(output_lstm)\n",
    "        output_middle_activated = F.tanh(output_middle)\n",
    "        output_sum = self.affin(output_middle_activated)\n",
    "        output_splitter_prob = F.softmax(output_sum,dim = 1)\n",
    "        output_splitter_prob = self.threshold(output_splitter_prob.view(output_splitter_prob.size(0), -1))\n",
    "        output_weighted = torch.bmm(output_splitter_prob.view(output_splitter_prob.size(0), 1, -1), output_lstm)\n",
    "        output_weighted = self.last_affin(output_weighted)\n",
    "        \n",
    "        return output_weighted, output_splitter_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder_4_k(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, output_dim):\n",
    "        super(GRUEncoder_4_k, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.middle_dim = 100 # middle dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(27, self.emb_dim) # a-z, paddingの合計27種類\n",
    "        self.gru = nn.LSTM(self.emb_dim, self.h_dim, 2, batch_first = True, dropout = 0.3, bidirectional = True)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.h_dim, self.middle_dim),\n",
    "            nn.Dropout(p = 0.3, inplace = True),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(self.middle_dim, 1)\n",
    "        )\n",
    "        self.last_affin = nn.Linear(self.h_dim * 2, self.h_dim)\n",
    "        \n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = torch.zeros(1 * 2 * 2, b_size, self.h_dim, device=device)\n",
    "        return (h0, h0)\n",
    "    \n",
    "    def forward(self, words, lengths, b_size):\n",
    "        \n",
    "        self.hidden, self.cell = self.init_hidden(words.size(0))\n",
    "        embed = self.emb(words)\n",
    "        lengths = lengths.view(-1)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        \n",
    "        #lstm\n",
    "        output, hidden = self.gru(packed_input, (self.hidden, self.cell))\n",
    "        output_lstm = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0] # batch * seq_len * hidden*2\n",
    "        \n",
    "        output_lstm_plus = output_lstm[:, :, :self.h_dim] + output_lstm[:, :, self.h_dim:]\n",
    "        \n",
    "        output_sum = self.main(output_lstm_plus)\n",
    "        output_splitter_prob = F.softmax(output_sum,dim = 1)\n",
    "        output_weighted = torch.bmm(output_splitter_prob.view(output_splitter_prob.size(0), 1, -1), output_lstm)\n",
    "        output_weighted = self.last_affin(output_weighted)\n",
    "        \n",
    "        return output_weighted, output_splitter_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ループを噛ませるとバッチを作るイテレータを生成する関数、ジェネレータを作る勉強\n",
    "# forループの中でいちいちバッチを作るより高速化したり、必要なメモリ量が減るらしい\n",
    "# 参考1: https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "# 参考2: https://www.lifewithpython.com/2015/11/python-create-iterator-protocol-class.html←こちらに準拠\n",
    "class DataIterator2(object):\n",
    "    def __init__(self, words, batch_len):\n",
    "        self.words = list(words)\n",
    "        self.n_words = len(words)\n",
    "        self.batch_len = batch_len\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.words) # 学習の各epoch毎に、単語を並び替える\n",
    "        for b_idx in range(0, self.n_words, self.batch_len):\n",
    "            word_batch = self.words[b_idx:b_idx+self.batch_len] # 単語のバッチを取り出す\n",
    "            \n",
    "            target_tensor = torch.from_numpy(np.array([embedding[word] for word in word_batch])).to(device) # (b, 300)\n",
    "            \n",
    "            word_batch = [[string.ascii_lowercase.index(l)+1 for l in word] for word in word_batch] # 各単語の文字をindex化する\n",
    "            word_lengths = torch.LongTensor([len(word) for word in word_batch], device=device) # 各単語の長さを測る\n",
    "            word_tensor = torch.zeros((len(word_batch), word_lengths.max()), device=device).long() \n",
    "            for w_idx, (word, w_len) in enumerate(zip(word_batch, word_lengths)): # paddingで単語の長さを揃える\n",
    "                word_tensor[w_idx, :w_len] = torch.LongTensor(word)\n",
    "            word_lengths, perm_idx = word_lengths.sort(0, descending=True) # 単語の長さを降順に並び替える\n",
    "            word_tensor = word_tensor[perm_idx] # バッチ内単語を単語の長さが長い順に並び替える\n",
    "            \n",
    "            yield word_tensor, word_lengths, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_batch(words, batch_len):\n",
    "    ''' 学習経過を観測するためのバッチ, trainとは別の単語群を用いる'''\n",
    "    word_batch = random.sample(words, batch_len)\n",
    "    target_tensor = torch.from_numpy(np.array([embedding[word] for word in word_batch])).to(device)      \n",
    "    word_batch = [[string.ascii_lowercase.index(l)+1 for l in word] for word in word_batch]\n",
    "    word_lengths = torch.LongTensor([len(word) for word in word_batch], device=device)\n",
    "    word_tensor = torch.zeros((len(word_batch), word_lengths.max()), device=device).long() \n",
    "    for w_idx, (word, w_len) in enumerate(zip(word_batch, word_lengths)):\n",
    "        word_tensor[w_idx, :w_len] = torch.LongTensor(word)\n",
    "    word_lengths, perm_idx = word_lengths.sort(0, descending=True)\n",
    "    word_tensor = word_tensor[perm_idx]\n",
    "    return word_tensor, word_lengths, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akihiro\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 27\n",
    "hidden_dim = 300\n",
    "output_dim = 300\n",
    "learning_rate = 0.01\n",
    "batch_len = 10\n",
    "n_epoch = 100\n",
    "print_every = 10\n",
    "\n",
    "encoder = GRUEncoder_2(emb_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adagrad(chain(encoder.parameters()), lr=learning_rate)\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "data_iterator = DataIterator2(train_words, batch_len)\n",
    "\n",
    "def train(train_iter, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    for i, (word_tensor, word_lengths, target_tensor) in enumerate(data_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        encoder_output, prob_vec = encoder(word_tensor,word_lengths, batch_len)\n",
    "        loss = criterion(encoder_output.view(-1, 300), target_tensor.view(-1, 300)) # (b, out_dim)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "    epoch_loss = epoch_loss / (i+1) # ループを最後まで回したから、iには(ループ回数-1)が代入されている\n",
    "    return epoch_loss\n",
    "\n",
    "def validate(validate_words, encoder,criterion, batch_len=100): # batch_len: 評価用の単語群、サンプリングする\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        word_tensor, word_lengths, target_tensor = generate_validation_batch(validate_words, batch_len)\n",
    "        encoder_outputs, _ = encoder(word_tensor, word_lengths, batch_len)\n",
    "        loss = criterion(torch.tensor(encoder_outputs), target_tensor)\n",
    "        valid_loss += loss\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e5dfa2087444d082e4ab148216bb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akihiro\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\akihiro\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 train_loss: 0.01228 valid_loss: 0.01232\n",
      "iter: 10 train_loss: 0.01218 valid_loss: 0.01225\n",
      "iter: 20 train_loss: 0.01218 valid_loss: 0.01190\n",
      "iter: 30 train_loss: 0.01217 valid_loss: 0.01221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-9bb15239c7be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidate_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-1a76629078e7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, optimizer, criterion)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (b, out_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_learning_curve = list()\n",
    "valid_learning_curve = list()\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    train_loss = train(data_iterator, optimizer, criterion)\n",
    "    valid_loss = validate(validate_words, encoder,criterion)\n",
    "    \n",
    "    train_learning_curve.append(train_loss)\n",
    "    valid_learning_curve.append(valid_loss)\n",
    "    if epoch % print_every == 0:\n",
    "        print(\"iter: {} train_loss: {:.5f} valid_loss: {:.5f}\".format(epoch, train_loss, valid_loss))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(train_learning_curve, label='train')\n",
    "ax.plot(valid_learning_curve, label='test')\n",
    "ax.set_ylim(0, 0.04)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_predict(word):\n",
    "    len_word = len(word)\n",
    "    word_tensor_input = DataIterator({word},1)\n",
    "    for i, (word_tensor, word_lengths, target_tensor, _) in enumerate(word_tensor_input):\n",
    "        encoder_output, prob_vec = encoder(word_tensor,word_lengths,1)\n",
    "        \n",
    "    return prob_vec[:len_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akihiro\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1231, 0.1218, 0.1132, 0.1712, 0.1000, 0.1182, 0.2524]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_predict('decided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
